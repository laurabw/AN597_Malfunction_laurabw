---
title: "Homework 4"
author: "Laura Brubaker-Wittman"
date: "10/24/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Part 1:

Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using
the following guidelines:

* Your function should take the following arguments: **p1** and **n1** (no default) representing the estimated
proportion and sample size (i.e., based on your sample data); **p2** and **n2** (both defaulting to NULL) that
contain a second sample’s proportion and sample size data in the event of a two-sample test; **p0** (no default)
as the expected value for the population proportion; and alternative (default “two.sided”) and **conf.level**
(default   0.95), to be used in the same way as in the function t.test().

* When conducting a two-sample test, it should be **p1** that is tested as being smaller or larger than **p2**
when alternative=“less” or alternative=“greater”, the same as in the use of x and y in the function t.test().

* The function should perform a one-sample Z-test using **p1**, **n1**, and **p0** if either p2 or n2 (or both) is
NULL.

* The function should contain a check for the rules of thumb we have talked about (*n* ∗ *p* > 5 and *n*  ∗
(1−**p) > 5) to ensure the validity of assuming the normal distribution in both the one- and two-sample settings.
If this is violated, the function should still complete but it should also print an appropriate warning message.

* The function should return a list containing the members **Z** (the test statistic), **P** (the appropriate p
value),and CI (the two-sided CI with respect to “conf.level” around **p1** in the case of a one-sample test and
around **p2-p1** in the case of a two-sample test). For all test alternatives (“two.sided”, “greater”, “less”),
calculate symmetric CIs based on quantiles of the normal distribution rather than worrying about calculating
single-limit confidence bounds.

Ok, so I worked on this for awhile and really got nowhere. I just did not have the time to figure it out. I
started by looking at what the t.test function involves, based on the Help menu:

    t.test(x, ...)

    ## Default S3 method:
    t.test(x, y = NULL,
       alternative = c("two.sided", "less", "greater"),
       mu = 0, paired = FALSE, var.equal = FALSE,
       conf.level = 0.95, ...)

    ## S3 method for class 'formula'
    t.test(formula, data, subset, na.action, ...)
    
I also looked up some resources on z-tests and scores, and while I get the concept, I am still feeling not 
confident at writing functions. I am going to go on with this homework and if I have time to come back I will. 
Hopefully my peer commentators will have some thoughts on this as well!

## Part 2

The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size.
For this exercise, the end aim is to fit a simple linear regression model to predict longevity
(**MaxLongevity_m**) measured in months from species’ brain size (**Brain_Size_Species_Mean**) measured in grams.
Do the following for both **longevity~brain size** and **log(longevity)~log(brain size)**:

* Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the
data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).

* Identify and interpret the point estimate of the slope (*β*1), as well as the outcome of the test associated
with the hypotheses H0: *β*1 = 0; HA: *β1* ≠ 0. Also, find a 90 percent CI for the slope (*β*1) parameter.

* Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a
legend to differentiate between the lines.

* Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800
gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or
why not?

* Looking at your two models, which do you think is better? Why?

Okay, so let's first load this data set:

```{r}
library(ggplot2)
library(dplyr)
library(curl)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall19/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```

Now, let's try to answer the questions for **longevity~brain size**, using what we learned in Modules 12 and 13:

*Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the
data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).*

```{r}
l_bs <- lm(MaxLongevity_m ~ Brain_Size_Species_Mean, data = d)
l_bs
names(l_bs)
l_bs$coefficients
head(l_bs$model)
```

Ok, have our model, now let's put it on a scatterplot:
```{r}
l_bs_plot <- ggplot(data = d, aes(x = MaxLongevity_m, y = Brain_Size_Species_Mean))
l_bs_plot <- l_bs_plot + geom_point()
l_bs_plot <- l_bs_plot + geom_smooth(method = "lm", formula = y ~ x)
l_bs_plot <- l_bs_plot + ggtitle("Linear Regression of Brain Size and Longetivty") + xlab("Longevity") + ylab("Brain Size (grams)")
l_bs_plot
```

This worked, though I had to look up again how to change labels. I worked for awhile with the geom_text()
function and could not get it to work how I wanted, though I do feel like I learned a lot about the function
itself. Will keep trying. On to the next step:

*Identify and interpret the point estimate of the slope (*β*1), as well as the outcome of the test associated
with the hypotheses H0: * *β* *1 = 0; HA: * *β1* *≠ 0. Also, find a 90 percent CI for the slope (* *β* *1)
parameter.*

```{r}
beta1 <- cor(d$Brain_Size_Species_Mean, d$MaxLongevity_m) * sd(d$Brain_Size_Species_Mean)/sd(d$MaxLongevity_m)
beta1
```

Ok, that cannot be right? Let's try this:

```{r}
l_bs <- lm(MaxLongevity_m ~ Brain_Size_Species_Mean, data = d)
l_bs
names(l_bs)
l_bs$coefficients
```

When we did this, we got the intercept, but not sure then how to find the slope? Sorry, just did not have enough
time to dedicate to this particular homework.

```{r}
ci <- confint(l_bs, level = 0.90)  # using the results of lm()
ci
```

Ok, this looks alright for confidence intervals? Thank goodness for built-in functions, am I right??

Now on to plotting CIs.

```{r}
m <- lm(data = d, d$Brain_Size_Species_Mean ~ d$MaxLongevity_m)
h_hat <- predict(m, newdata = data.frame(long = d$MaxLongevity_m))
df <- data.frame(cbind(d$MaxLongevity_m,d$Brain_Size_Species_Mean, h_hat))
names(df) <- c("x", "y", "yhat")
head(df)
g <- ggplot(data = df, aes(x = x, y = yhat))
g <- g + geom_point()
g <- g + geom_point(aes(x = x, y = y), colour = "red")
g <- g + geom_segment(aes(x = x, y = yhat, xend = x, yend = y))
g
```

Ok, this took awhile to figure out and still not sure if it is right? But similar to something we did in Module
12. Still need to learn how to do a legend!

Now try to find a point estimate and PIs for 800 grams specifically.

```{r}
pi <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "prediction", 
    level = 0.90)  # for a single value
pi
```

Ok, so in Module 12 this same process returned only 1 value, so not sure what is going on here? I must be missing
something. And not sure if these numbers are right.

Now we will try this all again with the log function added before each variable to transform the data. Here is the plot

```{r}
d$logBrainSize <- log(d$Brain_Size_Species_Mean)
d$logMaxLong <- log(d$MaxLongevity_m)
plot(data = d, logBrainSize ~ logMaxLong)
```

And let's do the model:

```{r}
log_model <- lm(data = d, logBrainSize ~ logMaxLong)
summary(log_model)
```

And the plot for this model:

```{r}
plot(log_model)
```

```{r}
logl_bs_plot <- ggplot(data = d, aes(x = logMaxLong, y = logBrainSize))
logl_bs_plot <- logl_bs_plot + geom_point()
logl_bs_plot <- logl_bs_plot + geom_smooth(method = "lm", formula = y ~ x)
logl_bs_plot <- logl_bs_plot + ggtitle("Linear Regression of Brain Size and Longetivty") + xlab("Longevity") + ylab("Brain Size (grams)")
logl_bs_plot
```

This plot looks like what we got with the untransformed data, so we can compare. It does make the relationship
look more strongly (positiviely) correlated, though both look linear.

Confidence intervals:

```{r}
ci <- confint(log_model, level = 0.90)  # using the results of lm()
ci
```

This looks okay, though still feel like I need to get better at what these numbers actually mean. That would help
with being able to gauge if my answers are correct or not.

Here are the plotted CIs:

```{r}
m <- lm(data = d, log(d$Brain_Size_Species_Mean) ~ log(d$MaxLongevity_m))
h_hat <- predict(m, newdata = data.frame(long = d$MaxLongevity_m))
df <- data.frame(cbind(log(d$MaxLongevity_m),log(d$Brain_Size_Species_Mean), h_hat))
names(df) <- c("x", "y", "yhat")
head(df)
g <- ggplot(data = df, aes(x = x, y = yhat))
g <- g + geom_point()
g <- g + geom_point(aes(x = x, y = y), colour = "red")
g <- g + geom_segment(aes(x = x, y = yhat, xend = x, yend = y))
g
```

Not sure if I can do the point estimate for 800 grams since it is transformed now?

```{r}
pi <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "prediction", 
    level = 0.90)  # for a single value
pi
```

Ok, it came up, but with 200 values again. Not sure how to fix that.

## Challenges:

This one was really tough for me! Which was frustrating as the last couple of homeworks, I really felt like I was
getting it. I am looking forward to seeing what my peers came up with and going over it in class.

1. Apparently I have no idea how to make a function as asked for in Part 1! This is a skill I need to work on as
I think it will be extremely helpful as I start evaluating my own data for my dissertation.

2. OVerall, just relearning my stats and knowing what all of this is actually telling me!

3. Had an issue with the PI giving me a ton of values though I thought I set it up for only one. Not sure how to
change this.

4. Learning how to put text and legends on graphics! This will come in handy, so will be teaching myself this and
asking questions about it.

## Peer Commentary Response
1. I ran into basically the same issues as you! The function was really difficult and I would love to go over how to approach something like it in class. 

2. Unfortunately, I can't really give many corrections since I had the same issues. 

3. I think the plethora of PI values has something to do with the NA coordinates still being used in the dataset. 
4. Your line of thinking is really easy to follow. Very nice annotations!

4. Something I learned was how to plot the residuals. 
